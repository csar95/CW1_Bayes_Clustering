{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing(val=True):\n",
    "    \n",
    "    data = pd.read_csv('data/x_train_gr_smpl.csv')\n",
    "    labels = pd.read_csv('data/y_train_smpl.csv')\n",
    "\n",
    "    if(val):\n",
    "        ## ---------------- Data preparation ---------------- ##\n",
    "        X_train = []\n",
    "        for i in range(data.shape[0]):\n",
    "            img = np.uint8(data.iloc[i])\n",
    "            edited = cv2.Canny(img, 10, 30)\n",
    "            edited = cv2.GaussianBlur(edited, (5, 5), 0)\n",
    "            X_train.append(edited.reshape((1,-1))[0])\n",
    "\n",
    "        data = pd.DataFrame(X_train)\n",
    "        ## -------------------------------------------------- ##\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 5: Deeper analysis of the data\n",
    "\n",
    "top_2_array, top_5_array, top_10_array = set(), set(), set()\n",
    "bottom_2_array, bottom_5_array, bottom_10_array = set(), set(), set()\n",
    "\n",
    "data, _ = preprocessing()\n",
    "\n",
    "for i in range(10):\n",
    "    data = data.reindex(np.arange(data.shape[0]))\n",
    "    labels = pd.read_csv(f'data/y_train_smpl_{i}.csv')\n",
    "    data['label'] = labels\n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    corr_label = data.drop(\"label\", axis=1).apply(lambda x: x.corr(data.label))\n",
    "    corr_label = [(index, abs(corr_val), i) for index, corr_val in enumerate(corr_label)]\n",
    "    corr_label = sorted(corr_label, key=lambda tup: tup[1], reverse=True)  # Order by correlation value\n",
    "            \n",
    "    for i, tup in enumerate(corr_label[:10]):\n",
    "        if i < 2:\n",
    "            top_2_array.add(tup[0])\n",
    "        if i < 5:\n",
    "            top_5_array.add(tup[0])\n",
    "        if i < 10:\n",
    "            top_10_array.add(tup[0])\n",
    "            \n",
    "    for i, tup in enumerate(corr_label[-10:]):\n",
    "        if i < 2:\n",
    "            bottom_2_array.add(tup[0])\n",
    "        if i < 5:\n",
    "            bottom_5_array.add(tup[0])\n",
    "        if i < 10:\n",
    "            bottom_10_array.add(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 6: Try to improve the classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "data, labels = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.46      0.53       462\n",
      "           1       0.54      0.35      0.42       621\n",
      "           2       0.41      0.58      0.48       157\n",
      "           3       0.73      0.88      0.80       431\n",
      "           4       0.86      0.75      0.80       675\n",
      "           5       0.85      0.76      0.80       717\n",
      "           6       0.49      0.84      0.62       267\n",
      "           7       0.23      0.89      0.36        75\n",
      "           8       0.86      0.82      0.84       668\n",
      "           9       0.57      0.43      0.49       105\n",
      "\n",
      "    accuracy                           0.68      4178\n",
      "   macro avg       0.61      0.67      0.61      4178\n",
      "weighted avg       0.71      0.68      0.68      4178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_top_2 = data[data.columns[list(top_2_array)]].copy(deep=True)\n",
    "data_top_2['label'] = labels\n",
    "\n",
    "X = data_top_2.iloc[:, :-1]  # All columns but the label\n",
    "y = data_top_2['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.58       482\n",
      "           1       0.57      0.36      0.44       594\n",
      "           2       0.40      0.73      0.51       129\n",
      "           3       0.76      0.91      0.83       442\n",
      "           4       0.86      0.79      0.83       703\n",
      "           5       0.87      0.81      0.84       713\n",
      "           6       0.67      0.86      0.76       247\n",
      "           7       0.33      0.88      0.49        77\n",
      "           8       0.90      0.84      0.87       699\n",
      "           9       0.64      0.78      0.70        92\n",
      "\n",
      "    accuracy                           0.73      4178\n",
      "   macro avg       0.66      0.75      0.69      4178\n",
      "weighted avg       0.75      0.73      0.73      4178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_top_5 = data[data.columns[list(top_5_array)]].copy(deep=True)\n",
    "data_top_5['label'] = labels\n",
    "\n",
    "X = data_top_5.iloc[:, :-1]\n",
    "y = data_top_5['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63       443\n",
      "           1       0.64      0.44      0.52       612\n",
      "           2       0.44      0.76      0.55       137\n",
      "           3       0.76      0.97      0.85       459\n",
      "           4       0.90      0.86      0.88       715\n",
      "           5       0.96      0.87      0.91       723\n",
      "           6       0.80      0.91      0.85       255\n",
      "           7       0.60      0.90      0.72        78\n",
      "           8       0.92      0.87      0.89       658\n",
      "           9       0.61      0.85      0.71        98\n",
      "\n",
      "    accuracy                           0.79      4178\n",
      "   macro avg       0.73      0.80      0.75      4178\n",
      "weighted avg       0.80      0.79      0.79      4178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_top_10 = data[data.columns[list(top_10_array)]].copy(deep=True)\n",
    "data_top_10['label'] = labels\n",
    "\n",
    "X = data_top_10.iloc[:, :-1]\n",
    "y = data_top_10['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[275,  84,  29,  14,   2,   1,  18,  10,   2,   8],\n",
       "       [137, 269,  91,  21,  16,   4,  40,   7,   9,  18],\n",
       "       [ 10,   7, 104,   2,   1,   0,   0,   2,   9,   2],\n",
       "       [  1,   9,   1, 443,   2,   0,   0,   0,   2,   1],\n",
       "       [  1,   8,   8,  47, 616,  22,   0,   5,   6,   2],\n",
       "       [  1,  12,   0,  12,  35, 627,   0,  12,  15,   9],\n",
       "       [  2,  15,   1,   4,   0,   0, 233,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   1,   0,   0,  70,   1,   6],\n",
       "       [  6,  15,   5,  38,   7,   1,   0,   7, 573,   6],\n",
       "       [  3,   0,   0,   1,   1,   0,   0,   3,   7,  83]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions (Draft)\n",
    "\n",
    "### Which streets signs are harder to recognise?\n",
    "Based on the recall measure labels 0 and 1 are the hardest to recognize.\n",
    "### Which street signs are most easily confused?\n",
    "Based on the confusion matrix the most confused signs are the label 0 and 1. \n",
    "### Which attributes (fields) are more reliable and which are less reliable in classification of street signs?\n",
    "By looking at top_2_array we can identify the most reliable features (pixels). By doing some operations like the module we can see those pixels are closer to the centre of the image (or at least don't belong to the borders).\n",
    "Regarding the less reliable pixels we consider the ones with the lowest correlation value with respect to the label.\n",
    "### What was the purpose of Tasks 5 and 6?\n",
    "Prepare the data in a way that we keep the most relevant attributes.\n",
    "### What would happen if the data sets you used in Tasks 4, 5 and 6 were not randomised?\n",
    "It doens't happen anything, the results are the same. TODO: Try with preprocessed data.\n",
    "### What would happen if there is cross-correlation between the non-class attributes?\n",
    "There is actually cross-correlation between pixels that are close to each other. As a consecuence of that, if two pixels are correlated (adjacent) to each other, and one of them is included in the top 10 of the most correlated pixels with the label, the other is likely to be in the top 10 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
