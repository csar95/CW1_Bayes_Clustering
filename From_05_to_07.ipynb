{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 5: Deeper analysis of the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "top_2_array = set()\n",
    "top_5_array = set()\n",
    "top_10_array = set()\n",
    "\n",
    "bottom_2_array = set()\n",
    "bottom_5_array = set()\n",
    "bottom_10_array = set()\n",
    "\n",
    "data = pd.read_csv('data/x_train_gr_smpl.csv')\n",
    "for i in range(10):\n",
    "    data = data.reindex(np.arange(data.shape[0]))\n",
    "    labels = pd.read_csv(f'data/y_train_smpl_{i}.csv')\n",
    "    data['label'] = labels\n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    corr_label = data.drop(\"label\", axis=1).apply(lambda x: x.corr(data.label))\n",
    "    corr_label = [(index, abs(corr_val), i) for index, corr_val in enumerate(corr_label)]\n",
    "    corr_label = sorted(corr_label, key=lambda tup: tup[1], reverse=True)  # Order by correlation value\n",
    "            \n",
    "    for i, tup in enumerate(corr_label[:10]):\n",
    "        if i < 2:\n",
    "            top_2_array.add(tup[0])\n",
    "        if i < 5:\n",
    "            top_5_array.add(tup[0])\n",
    "        if i < 10:\n",
    "            top_10_array.add(tup[0]) #, tup[2]))\n",
    "            \n",
    "    for i, tup in enumerate(corr_label[-10:]):\n",
    "        if i < 2:\n",
    "            bottom_2_array.add(tup[0])\n",
    "        if i < 5:\n",
    "            bottom_5_array.add(tup[0])\n",
    "        if i < 10:\n",
    "            bottom_10_array.add(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = sorted([(int(round(x/48)), x % 48) for x in bottom_2_array])\n",
    "# a = a.filter(lambda (x, y): x >10 or x < 37)\n",
    "# print(a)\n",
    "# b = sorted([x % 48 for x in bottom_5_array])\n",
    "# print(b)\n",
    "# c = sorted([(int(round(x[0]/48)), x[0] % 48, x[1]) for x in top_10_array])\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 6: Try to improve the classification\n",
    "\n",
    "data = pd.read_csv('data/x_train_gr_smpl.csv')\n",
    "labels = pd.read_csv('data/y_train_smpl.csv')\n",
    "\n",
    "data_top_2 = data[data.columns[list(top_2_array)]].copy(deep=True)\n",
    "data_top_2['label'] = labels\n",
    "# data_top_2 = data_top_2.sample(frac=1)\n",
    "\n",
    "data_top_5 = data[data.columns[list(top_5_array)]].copy(deep=True)\n",
    "data_top_5['label'] = labels\n",
    "# data_top_5 = data_top_5.sample(frac=1)\n",
    "\n",
    "data_top_10 = data[data.columns[list(top_10_array)]].copy(deep=True)\n",
    "data_top_10['label'] = labels\n",
    "# data_top_10 = data_top_10.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.39      0.30       496\n",
      "           1       0.28      0.20      0.24       626\n",
      "           2       0.12      0.86      0.21       140\n",
      "           3       0.85      0.54      0.66       422\n",
      "           4       0.55      0.41      0.47       705\n",
      "           5       0.88      0.53      0.66       687\n",
      "           6       0.47      0.63      0.54       269\n",
      "           7       0.31      0.38      0.34        84\n",
      "           8       0.49      0.06      0.11       662\n",
      "           9       0.24      0.48      0.32        87\n",
      "\n",
      "   micro avg       0.38      0.38      0.38      4178\n",
      "   macro avg       0.44      0.45      0.38      4178\n",
      "weighted avg       0.52      0.38      0.40      4178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "X = data_top_2.iloc[:, :-1]  # All columns but the label\n",
    "y = data_top_2['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.37      0.29       472\n",
      "           1       0.30      0.21      0.24       637\n",
      "           2       0.12      0.90      0.21       151\n",
      "           3       0.94      0.59      0.72       440\n",
      "           4       0.62      0.36      0.46       702\n",
      "           5       0.91      0.47      0.62       700\n",
      "           6       0.51      0.61      0.56       250\n",
      "           7       0.18      0.42      0.25        78\n",
      "           8       0.66      0.11      0.18       635\n",
      "           9       0.20      0.42      0.27       113\n",
      "\n",
      "   micro avg       0.38      0.38      0.38      4178\n",
      "   macro avg       0.47      0.45      0.38      4178\n",
      "weighted avg       0.57      0.38      0.41      4178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data_top_5.iloc[:, :-1]\n",
    "y = data_top_5['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.38      0.30       469\n",
      "           1       0.32      0.21      0.25       590\n",
      "           2       0.11      0.89      0.19       131\n",
      "           3       0.98      0.59      0.73       469\n",
      "           4       0.67      0.35      0.46       700\n",
      "           5       0.74      0.49      0.59       709\n",
      "           6       0.54      0.65      0.59       263\n",
      "           7       0.14      0.39      0.20        79\n",
      "           8       0.65      0.12      0.20       664\n",
      "           9       0.29      0.51      0.37       104\n",
      "\n",
      "   micro avg       0.39      0.39      0.39      4178\n",
      "   macro avg       0.47      0.46      0.39      4178\n",
      "weighted avg       0.57      0.39      0.42      4178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data_top_10.iloc[:, :-1]\n",
    "y = data_top_10['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178,  45, 186,   0,  10,   0,  21,  19,   4,   6],\n",
       "       [141, 123, 237,   0,  15,   2,  11,  40,   1,  20],\n",
       "       [  3,   2, 116,   0,   0,   0,   0,  10,   0,   0],\n",
       "       [ 50,  32,  66, 275,  25,  10,   2,   0,   1,   8],\n",
       "       [104,  35, 155,   3, 243,   6,  10, 100,  14,  30],\n",
       "       [114,  56,  78,   3,   0, 346,  35,   1,  20,  56],\n",
       "       [ 32,  17,  11,   0,   4,  15, 171,   4,   3,   6],\n",
       "       [  8,  22,   3,   0,   8,   0,   5,  31,   0,   2],\n",
       "       [ 97,  31, 229,   0,  60,  89,  53,  23,  80,   2],\n",
       "       [  9,  17,  17,   0,   0,   0,   8,   0,   0,  53]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions (Draft)\n",
    "\n",
    "### Which streets signs are harder to recognise?\n",
    "Based on the recall measure labels 1 and 8 are the harsdest to recognize.\n",
    "### Which street signs are most easily confused?\n",
    "Based on the confusion matrix the most confused sign is the label 2 and, in order, it's mistaken for labels 1, 8, 0 and 4. \n",
    "### Which attributes (fields) are more reliable and which are less reliable in classification of street signs?\n",
    "By looking at top_2_array we can identify the most reliable features (pixels). By doing some operations like the module we can see those pixels are closer to the centre of the image (or at least don't belong to the borders).\n",
    "Regarding the less reliable pixels we consider the ones with the lowest correlation value with respect to the label.\n",
    "### What was the purpose of Tasks 5 and 6?\n",
    "Prepare the data in a way that we keep the most relevant attributes.\n",
    "### What would happen if the data sets you used in Tasks 4, 5 and 6 were not randomised?\n",
    "It doens't happen anything, the results are the same.\n",
    "### What would happen if there is cross-correlation between the non-class attributes?\n",
    "There is actually cross-correlation between pixels that are close to each other. As a consecuence of that, if two pixels are correlated (adjacent) to each other, and one of them is included in the top 10 of the most correlated pixels with the label, the other is likely to be in the top 10 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
